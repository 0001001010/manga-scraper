# Manga Image Scraper

Um spider Scrapy eficiente para baixar mangÃ¡s pt-br, com suporte a coleta, downloads e monitoramento de atualizaÃ§Ãµes.

## Funcionalidades

- ğŸ” **Coleta de SÃ©ries**: Mapeia todas as sÃ©ries disponÃ­veis
- ğŸ“¥ **Download em Lote**: Baixa mÃºltiplas sÃ©ries
- ğŸ”„ **Sistema de AtualizaÃ§Ã£o**: Verifica e baixa novos capÃ­tulos
- ğŸ’¾ **Cache Inteligente**: Evita downloads redundantes
- ğŸ“Š **Monitoramento**: Logs detalhados e relatÃ³rios de progresso

## Requisitos

### Python

- Python 3.9 ou superior

### DependÃªncias

Instale todas as dependÃªncias necessÃ¡rias usando:

```bash
pip install -r requirements.txt
```

### Principais DependÃªncias

- **scrapy**: Framework de web scraping
- **pillow**: Processamento de imagens
- **scrapy-user-agents**: RotaÃ§Ã£o de User-Agents
- **tqdm**: Barras de progresso para downloads

### DependÃªncias de Desenvolvimento

- **black**: FormataÃ§Ã£o de cÃ³digo
- **pylint**: AnÃ¡lise estÃ¡tica
- **mypy**: VerificaÃ§Ã£o de tipos
- **pyflakes**: VerificaÃ§Ã£o de cÃ³digo

### Ambiente Virtual (Recomendado)

```bash
# Criar ambiente virtual
python -m venv venv

# Ativar ambiente virtual
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

### Verificando a InstalaÃ§Ã£o

```bash
python -c "import scrapy; print(scrapy.__version__)"
```

Deve exibir a versÃ£o do Scrapy instalada.

Esta seÃ§Ã£o de requisitos mais detalhada no README ajuda os usuÃ¡rios a:

1. Entender todas as dependÃªncias necessÃ¡rias
2. Configurar um ambiente virtual (prÃ¡tica recomendada)
3. Verificar se a instalaÃ§Ã£o foi bem-sucedida
4. Identificar quais pacotes sÃ£o para desenvolvimento

AlÃ©m disso, o `requirements.txt` especifica as versÃµes mÃ­nimas recomendadas para cada pacote, garantindo compatibilidade e funcionamento adequado do projeto.

## Estrutura do Projeto

```

scrapy/
â”œâ”€â”€ scraper/
â”‚ â”œâ”€â”€ spiders/
â”‚ â”‚ â”œâ”€â”€ **init**.py
â”‚ â”‚ â”œâ”€â”€ image_spider.py
â”‚ â”‚ â””â”€â”€ series_spider.py
â”‚ â”œâ”€â”€ **init**.py
â”‚ â”œâ”€â”€ items.py
â”‚ â”œâ”€â”€ middlewares.py
â”‚ â”œâ”€â”€ pipelines.py
â”‚ â””â”€â”€ settings.py
â”œâ”€â”€ downloads/ # DiretÃ³rio onde os mangÃ¡s sÃ£o salvos
â””â”€â”€ cache/ # Cache e logs do sistema

```

## Como Usar

### 1. Coletar SÃ©ries DisponÃ­veis

```bash
scrapy crawl series_spider -a mode=collect
```

Este comando mapeia todas as sÃ©ries disponÃ­veis e as armazena no cache.

### 2. Baixar SÃ©ries

```bash
scrapy crawl series_spider -a mode=download
```

### 3. Verificar AtualizaÃ§Ãµes

```bash
scrapy crawl series_spider -a mode=update
```

Verifica e baixa novos capÃ­tulos de sÃ©ries jÃ¡ baixadas.

## Estrutura dos Downloads

```
downloads/
â”œâ”€â”€ Nome_da_Serie_1/
â”‚   â”œâ”€â”€ Capitulo_1/
â”‚   â”‚   â”œâ”€â”€ pagina_001.jpg
â”‚   â”‚   â”œâ”€â”€ pagina_002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Capitulo_2/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Nome_da_Serie_2/
â””â”€â”€ ...
```

## Cache e Logs

O sistema mantÃ©m vÃ¡rios arquivos de controle em `cache/`:

- `series_cache.json`: Lista de sÃ©ries disponÃ­veis
- `download_progress.json`: Progresso dos downloads
- `update_log.json`: Registro de atualizaÃ§Ãµes
- `error_log.json`: Log de erros
- `stats_*.json`: EstatÃ­sticas de execuÃ§Ã£o

## Monitoramento

### Logs em Tempo Real

```bash
tail -f cache/spider_*.log
```

### RelatÃ³rios

ApÃ³s cada execuÃ§Ã£o, um relatÃ³rio detalhado Ã© gerado em:

```
cache/report/report_[mode]_[timestamp].json
```

## Retomada de Downloads

O sistema mantÃ©m o estado dos downloads, permitindo retomar de onde parou em caso de interrupÃ§Ã£o.

## Tratamento de Erros

- Retry automÃ¡tico em caso de falhas
- Log detalhado de erros
- Backup automÃ¡tico do cache

## ConfiguraÃ§Ãµes AvanÃ§adas

Edite `settings.py` para ajustar:

- Delays entre requisiÃ§Ãµes
- Timeouts
- ConfiguraÃ§Ãµes de proxy
- Headers personalizados
- Cache de requisiÃ§Ãµes

## ResoluÃ§Ã£o de Problemas

1. **Cache Corrompido**
   - O sistema usa backups automÃ¡ticos
   - Apague manualmente arquivos corrompidos em `cache/`

## Contribuindo

1. Fork o repositÃ³rio
2. Crie sua branch de feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## Notas

- Configure delays apropriados para evitar sobrecarga
- Mantenha backups dos arquivos importantes

## Suporte

Para problemas, dÃºvidas ou sugestÃµes, abra uma issue no repositÃ³rio.
